<!doctype html>
<html lang="en">

<head>
  <meta name="google-site-verification" content="ftFOlJETX-2KNjaPh8W6s8lhigItRuu9fOmjHZZ0nY0" />
  <!-- Required meta tags -->
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <!-- Bootstrap CSS -->
  <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/css/bootstrap.min.css"
    integrity="sha384-MCw98/SFnGE8fJT3GXwEOngsV7Zt27NXFoaoApmYm81iuXoPkFOJwJ8ERdknLPMO" crossorigin="anonymous">

  <title>HF-RealtimeEVC</title>
</head>
<style type="text/css">
  table {
    width: 100%;
    table-layout: fixed;
  }

  audio {
    width: 100%;
  }

  thead>tr>th:first-child {
    width: 96px;
  }

  @media (max-width: 767px) {
    .big-screen {
      display: none;
    }
  }

  @media (min-width: 767px) {
    .small-screen {
      display: none;
    }
  }
</style>




<body>
  <header class="header">
    <div class="jumbotron bg-secondary text-center">
      <div class="container">
        <div class="row align-items-center">
          <div class="col-md-12">
 
            <h1><a class="text-light">Decoupling Speaker-Independent Emotions for Voice Conversion Via Source-Filter Networks</h1><br> 
             <font size=5><span style="color:#000000"> Authors: Zhaojie Luo, Shoufeng Lin, Rui Liu, Jun Baba, Yuichiro Yoshikawa, Ishiguro Hiroshi </font>  </a>   
            <p>
              <div class="row">
              </div>
            </p>
            
          </div>
        </div>
      </div>
    </div>
  </header>
  <main>
    <div class="container">
      <div class="row" id="result">
        <div class="col-md-12">
        	<h5>Abstract:</h5> 
        	Emotional voice conversion (VC) aims to convert a neutral voice to an emotional (e.g. happy) one while retaining the linguistic information and speaker identity. We note that the decoupling of emotional features from other speech information (such as speaker, content, etc.) is the key to achieving remarkable performance. Some recent attempts about speech representation decoupling on the neutral speech can not work well on the emotional speech, due to the more complex acoustic properties involved in the latter. To address this problem, here we propose a novel Source-Filter-based Emotional VC model (SFEVC) to achieve proper filtering of speaker-independent emotion features from both the timbre and pitch features. Our SFEVC model consists of multi-channel encoders, emotion separate encoders, and one decoder. Note that all encoder modules adopt a designed information bottlenecks auto-encoder. Additionally, to further improve the conversion quality for various emotions, a novel two-stage training strategy based on the 2D Valence-Arousal (VA) space was proposed. Experimental results show that the proposed SFEVC along with a two-stage training strategy outperforms all baselines and achieves the state-of-the-art performance in speaker-independent emotional VC with nonparallel data. 
		<br>
<br>
<br>
          <div style="text-align: center; width: 1100px; border: green solid 1px;">
          	<img align="center" src="fig01.png"  width="950" height="255"  />
 				<br>
        <br>
                    Fig. 1: Emotional speech information and source-filter networks.
                    
        
          </div> 
		<br />
                     <h3>Speech Samples:</h3>
                     <br />
            
                   
                     </thead> <b>Experimental Setup:</b><p>
                  
                    <h6> (1) <b>DBNs+NNs [1] </b>: This is the earliest emotional VC method based on deep learning models. The model uses the DBNs to convert spectral features while using the NNs to convert the F0 features. </h6>
                    <h6>(2) <b>Dual-SANs [2] </b>: This model adopts the dual supervised adversarial network, in which continuous wavelet transform method was used to augment prosody (F0) features. </h6>
                    <h6>(3) <b>CycleGAN [3] </b>: The CycleGAN model has been widely used in the non-parallel VC tasks. Kun et al. have also used this unsupervised learning model in the emotional VC. </h6> 
	            <h6>(4) <b>SPEECHFLOW [4]  </b>: This is a state-of-the-art source filter method that has been used in the voice conversion. We applied it in the emotional VC that uses the designed bottlenecks auto-encoder for filtering the timbre and prosody features. </h6>  
	            <h6>(5) <b>SFEVC </b>: This is our proposed method SFEVC that uses designed bottlenecks multi-channel encoders and the emotion-separate encoders, while without two-stage training. </h6>   
                     <br />
	             <table class="table">
                      <thead>
                            <th colspan=5><h4>Speech Naturalness Evaluation</h4></th>
                        <tr>
             
                      
                        </tr>

                      <table style="height: 100px;" border="2" width="611">
                      <tbody>
                                
                                
                                <tr>
                                    
                                    <td style="text-align: center; width: 100px;">  Source</th>
                                    <td style="text-align: center; width: 100px;">  DBNs+NNs</th>
                                    <td style="text-align: center; width: 100px;">  Dual-SANs</th>
                                    <td style="text-align: center; width: 100px;">  CycleGAN </th>
			            <td style="text-align: center; width: 100px;">  SPEECHFLOW </th>
	                            <td style="text-align: center; width: 100px;">  SFEVC </th>	     
                                    <td style="text-align: center; width: 100px;">  Target</th>
                          
                                </tr> 
                                  <!--  single sentence -->
                                                          <tr>
             
                            <th colspan=3><h8>(1) Neutral-to-Happy</h8></th>
                        </tr>
                                  <td>
                                    <audio controls>
                                      <source src="./samples/source/N2H.wav" type="audio/wav" />
                                      Your browser does not support the audio element.
                                    </audio>
                                
                                  </td>
                                  <td>
                                    <audio controls>
                                      <source src="./samples/DBN+NNs/N2H.wav"  type="audio/wav" />
                                      Your browser does not support the audio element.
                                    </audio>
                                  </td>
                                  <td>
                                    <audio controls>
                                      <source src="./samples/dual-SANs/N2H.wav"  type="audio/wav" />
                                      Your browser does not support the audio element.
                                    </audio>
			         </td>    
                                 <td>
                                    <audio controls>
                                      <source src="./samples/cycleGAN/N2H.wav" type="audio/wav" />
                                      Your browser does not support the audio element.
                                    </audio>
                                  </td>    
                                 <td>
                                    <audio controls>
                                      <source src="./samples/SPEECHFLOW/N2H.wav" type="audio/wav" />
                                      Your browser does not support the audio element.
                                    </audio>
                                  </td>
                                   <td>
			          <audio controls>
                                      <source src="./samples/SFEVC/N2H.wav" type="audio/wav" />
                                      Your browser does not support the audio element.
                                    </audio>
                                  </td>
                                   <td>
                                   <audio controls>
                                      <source src="./samples/target01/N2H.wav"  type="audio/wav" />
                                      Your browser does not support the audio element.
                                    </audio>
                                  </td>                      
                                  </tr>
                     
                      </tbody>                   

                    </table>
             <hr/>
            <br/>
            <br/>    
          <h5>References</h5>
	  </br>  
          [1] Zhaojie Luo, Tetsuya Takiguchi, and Yasuo Ariki, “Emotional voice conversion using deep neural networks with mcc and f0 features,” in 2016 IEEE/ACIS 15th International Conference on Computer and Information Science (ICIS). IEEE, 2016, pp. 1–5.
          <br/>
          [2] Zhaojie Luo, Jinhui Chen, Tetsuya Takiguchi, and Yasuo Ariki, “Emotional voice conversion using dual supervised adversarial networks with continuous wavelet transform f0 features,” IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 27, no. 10, pp. 1535–1548, 2019.
          <br/>
          [3] K. Zhou, B. Sisman, and H. Li, “Transforming spectrum and prosody for emotional voice conversion with non-parallel training data,” arXiv preprint arXiv:2002.00198, 2020. 
          <br/>           
          [4] K. Qian, Y. Zhang, S. Chang, M. Hasegawa-Johnson, and D. Cox, “Unsupervised speech decomposition via triple information bottleneck,” in International Conference on Machine Learning. PMLR, 2020, pp. 7836–7846.
          <br>
           
           </div>
          </div>
	 </div>  
		 
    
      <hr>
      <!-- <div class="row" id="contact">
          <div class="col">
            <h2>Contact</h2>
            <div></div>
          </div>
        </div> -->
    </div>
  </main>
  <footer class="bg-secondary text-light mt-4 pt-3 pb-2 ">
    <div class="container">
      <p class="text-center">
        
      </p>
    </div>
  </footer>
  <!-- Optional JavaScript -->
  <!-- jQuery first, then Popper.js, then Bootstrap JS -->
  <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js"
    integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo"
    crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js"
    integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49"
    crossorigin="anonymous"></script>
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/js/bootstrap.min.js"
    integrity="sha384-ChfqqxuZUCnJSK3+MXmPNIyE6ZbWh2IMqE241rYiqJxyMiZ6OW/JmZQ5stwEULTy"
    crossorigin="anonymous"></script>
</body>

</html>
